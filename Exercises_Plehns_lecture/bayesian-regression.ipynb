{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks for Amplitude Regression - Student Version\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial introduces **Bayesian Neural Networks (BNNs)** applied to amplitude regression in particle physics. Unlike traditional neural networks that provide point estimates, Bayesian neural networks capture uncertainty in both predictions and model parameters.\n",
    "\n",
    "### What You'll Learn\n",
    "- How to implement variational inference in neural networks\n",
    "- The reparameterization trick for gradient-based optimization\n",
    "- KL divergence regularization in Bayesian models\n",
    "- Practical applications to particle physics amplitude regression\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Bayesian Neural Networks**: Instead of learning deterministic weights, BNNs learn distributions over weights, enabling uncertainty quantification.\n",
    "\n",
    "**Variational Inference**: An approximate method to perform Bayesian inference by optimizing a variational distribution to approximate the true posterior.\n",
    "\n",
    "**Reparameterization Trick**: A technique to make stochastic nodes differentiable by expressing random variables as deterministic functions of noise.\n",
    "\n",
    "### Prerequisites\n",
    "- Basic knowledge of neural networks\n",
    "- Familiarity with PyTorch\n",
    "- Understanding of probability distributions\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "\n",
    "We'll work with synthetic amplitude data that simulates particle physics experiments. The goal is to predict amplitudes based on input features while quantifying our uncertainty about these predictions. \n",
    "We want to train a network to predict the amplitude for the process $gg \\rightarrow \\gamma \\gamma g$. So the amplitude depends on the 4-momentum of 5 particles: 2 incoming gluons, 2 outgoing photons, and one outgoing gluon. The incoming gluons will have no transverse momentum, but their total momentum along the beam pipe is not necessarily zero.\n",
    "\n",
    "The dataset contains:\n",
    "- **Input features**: 5 particles' 4-momenta (20 features)\n",
    "- **Target amplitudes**: Real-valued outputs representing physical amplitudes\n",
    "\n",
    "This is an ideal scenario for Bayesian methods since we want to understand not just what the amplitude is, but how confident we are in our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.colors as mcolors\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the get_data.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to download dataset 1 ('amplitude regression', used in tutorials [2, 3, 4]) from https://www.thphys.uni-heidelberg.de/~plehn/data/tutorial-2-data.zip to tutorial-2-data.zip\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 39.73it/s]\n",
      "Successfully extracted files from tutorial-2-data.zip\n",
      "get_data.py             tutorial-2-data (1).zip tutorial-2-data (3).zip\n",
      "\u001b[1m\u001b[36mtutorial-2-data\u001b[m\u001b[m         tutorial-2-data (2).zip tutorial-2-data.zip\n"
     ]
    }
   ],
   "source": [
    "# Download data first if needed\n",
    "!python3 data/get_data.py 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat = np.load(\"data/tutorial-2-data/trn_dat.npy\")\n",
    "trn_amp = np.load(\"data/tutorial-2-data/trn_amp.npy\")\n",
    "\n",
    "val_dat = np.load(\"data/tutorial-2-data/val_dat.npy\")\n",
    "val_amp = np.load(\"data/tutorial-2-data/val_amp.npy\")\n",
    "\n",
    "tst_dat = np.load(\"data/tutorial-2-data/tst_dat.npy\")\n",
    "tst_amp = np.load(\"data/tutorial-2-data/tst_amp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (30000, 5, 4)\n",
      "train amp  shape: (30000,)\n",
      "test  data shape: (30000, 5, 4)\n",
      "test  amp  shape: (30000,)\n",
      "val   data shape: (30000, 5, 4)\n",
      "val   amp  shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data shape: {trn_dat.shape}\")\n",
    "print(f\"train amp  shape: {trn_amp.shape}\")\n",
    "print(f\"test  data shape: {tst_dat.shape}\")\n",
    "print(f\"test  amp  shape: {tst_amp.shape}\")\n",
    "print(f\"val   data shape: {val_dat.shape}\")\n",
    "print(f\"val   amp  shape: {val_amp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pt(fv):\n",
    "    \"\"\" returns p_T of given four vector \"\"\"\n",
    "    ptsq = np.round(fv[:, 1]**2 + fv[:, 2]**2, 5)\n",
    "    return np.sqrt(ptsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps (similar to previous tutorials)\n",
    "trn_dat_gluon_pt = get_pt(trn_dat[:, 4])\n",
    "\n",
    "nev = trn_dat.shape[0]\n",
    "trn_datf = np.reshape(trn_dat, (nev,-1))\n",
    "val_datf = np.reshape(val_dat, (nev,-1))\n",
    "tst_datf = np.reshape(tst_dat, (nev,-1))\n",
    "\n",
    "gpt = np.mean(trn_dat_gluon_pt)\n",
    "trn_datf = trn_datf / gpt\n",
    "val_datf = val_datf / gpt\n",
    "tst_datf = tst_datf / gpt\n",
    "\n",
    "trn_datfp = torch.Tensor(trn_datf)\n",
    "val_datfp = torch.Tensor(val_datf)\n",
    "tst_datfp = torch.Tensor(tst_datf)\n",
    "\n",
    "trn_ampl = np.log(trn_amp)\n",
    "val_ampl = np.log(val_amp)\n",
    "tst_ampl = np.log(tst_amp)\n",
    "\n",
    "trn_amplp = torch.Tensor(trn_ampl)\n",
    "val_amplp = torch.Tensor(val_ampl)\n",
    "tst_amplp = torch.Tensor(tst_ampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class amp_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, amp):\n",
    "        self.data = data\n",
    "        self.amp = amp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.amp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.amp[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset = amp_dataset(trn_datfp, trn_amplp.unsqueeze(-1))\n",
    "val_dataset = amp_dataset(val_datfp, val_amplp.unsqueeze(-1))\n",
    "tst_dataset = amp_dataset(tst_datfp, tst_amplp.unsqueeze(-1))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variational Bayesian Linear Layer\n",
    "\n",
    "The core of our Bayesian neural network is the **VBLinear** layer. Unlike standard linear layers that have deterministic weights, this layer treats weights as probability distributions.\n",
    "\n",
    "### Key Components:\n",
    "1. **Mean parameters** (μ): The central tendency of weight distributions\n",
    "2. **Rho parameters** (ρ): Related to the variance via σ = log(1 + exp(ρ))\n",
    "3. **Reparameterization**: w = μ + σ ⊙ ε, where ε ~ N(0,1)\n",
    "4. **KL divergence**: Regularization term measuring distance from prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the source code for a **basic linear layer** in pytorch:\n",
    "\n",
    "(https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Linear(Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects of this class apply a linear transformation to the incoming data: $y = xA^T + b$.\n",
    "\n",
    "The input arguments are required to initialise the layer, so, in the \\_\\_init()\\_\\_ function.  We have:\n",
    "- in\\_features: size of each input sample\n",
    "- out\\_features: size of each output sample\n",
    "- bias: If set to ``False``, the layer will not learn an additive bias.  Default: ``True``\n",
    "\n",
    "The shapes are:\n",
    "- Input: $(*, H_{in})$ where $*$ means any number of dimensions including none and $H_{in} = \\text{in\\_features}$.\n",
    "- Output: $(*, H_{out})$ where all but the last dimension are the same shape as the input and $H_{out} = \\text{out\\_features}$.\n",
    "\n",
    "The layer has attributes:\n",
    "- weight: the learnable weights of the module of shape $(\\text{out\\_features}, \\text{in\\_features})$. The values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{\\text{in\\_features}}$\n",
    "- bias:   the learnable bias of the module of shape $(\\text{out\\_features})$.  If `bias` is ``True``, the values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{\\text{in\\_features}}$.\n",
    "\n",
    "Examples::\n",
    "\n",
    "    >>> m = nn.Linear(20, 30)\n",
    "    \n",
    "    >>> input = torch.randn(128, 20)\n",
    "    \n",
    "    >>> output = m(input)\n",
    "    \n",
    "    >>> print(output.size())\n",
    "    \n",
    "    torch.Size([128, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture, we know that in a Bayesian network the weights are replaced by Gaussian distributions, and on a forward pass we get the output by sampling from that distribution.\n",
    "\n",
    "So the biases are the same as in the linear layer.  But not each weight is a Gaussian distibution and so needs a mean and a variance.  The bias and the mean and variance of the weight distribution will be learnable.  In practice it's easier to work with the log of the variance as it is more stable when optimising the network.\n",
    "\n",
    "We need to be able to sample from the Gaussian weight distributions, and compute derivatives of the output in order to update the network parameters.  To do this we use something called the 're-parameterisation trick'.  It involves sampling random noise from a unit normal distribution, and then transforming that number using the mean and variance of the weight distribution in this way:\n",
    "\\begin{equation}\n",
    "w = \\mu + \\sigma\\times r\n",
    "\\end{equation}\n",
    "where $r$ is a random number sampled from a unit normal distribution (Gaussian with mean and variance equal to one), $\\mu$ is the mean of the weight distribution, and $\\sigma$ is the standard deviation.  In this way we separate the stochastic part of the function from the parameters defining the distribution.  And so if we take any differentiable function of $x$ (e.g. an activation function), we can compute derivatives of that function with respect to the mean and variance of the weight distribution.\n",
    "\n",
    "In the `forward` method we then need to implement this reparameterisation trick for the weights, and then apply the same linear transformation as in the standard linear layer.\n",
    "\n",
    "On each forward pass we need to generate a set of random numbers with the same shape as our means and variances.  Choosing a set of random numbers for the sampling is equivalent to 'sampling' a new neural network from the Bayesian neural network.  And sometimes at the end of the analysis, we will want to keep the same network for testing.  So we don't always want to re-sample the random numbers on each forward pass.  To control this we define a flag called `self.resample`, with default set to ``True``.\n",
    "\n",
    "We also need a `reset_parameters` function to reset the parameters in the network.  This is standard for layers in pytorch.  We use a slightly different function to do this than is used in the pytorch linear layer, as can be seen below.\n",
    "\n",
    "From the lecture, you know that the weight distributions require a prior.  The simplest choice for this prior is just a Gaussian distribution with a mean and variance of one.  Results are typically not too sensitive to this prior, as long as the values are within reasonable limits.  For example, $\\mathcal{O}(1)$ means and standard deviations.  Going beyond $\\mathcal{O}(1)$ numbers just leads to numerical instabilities in the training.  The Bayesian loss function contains a term coming from the KL divergence between the weight distributions in the network and their priors.  So the layers should have some funcitonality to return these values.  The KL divergence for the layer is:\n",
    "\\begin{equation}\n",
    "\\text{KL} = \\sum_{\\text{weights}} 0.5 \\times (  \\mu^2 + \\sigma^2 - \\log\\sigma^2 - 1 )\n",
    "\\end{equation}\n",
    "\n",
    "So let's build the **simplest Bayesian layer** we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement the VBLinear Layer\n",
    "\n",
    "Your task is to complete the implementation of the Variational Bayesian Linear layer. This layer is the building block of Bayesian neural networks.\n",
    "\n",
    "**Instructions:**\n",
    "1. Complete the `__init__` method to initialize the required parameters\n",
    "2. Implement the `forward` method using the reparameterization trick\n",
    "3. Implement the `reset_parameters` method for proper initialization\n",
    "4. Complete the `KL` method to compute KL divergence\n",
    "\n",
    "**Hints:**\n",
    "- Use `Parameter(Tensor(...))` for learnable parameters\n",
    "- The reparameterization trick: `weight = μ + σ * ε` where `ε ~ N(0,1)`\n",
    "- KL divergence formula: `0.5 * (μ² + σ² - log(σ²) - 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLinear(Module):\n",
    "    # VB -> Variational Bayes\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(VBLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.resample = True\n",
    "        \n",
    "        # TODO: Initialize the parameters\n",
    "        # You need:\n",
    "        # - bias: Parameter for bias terms\n",
    "        # - mu_w: Parameter for weight means\n",
    "        # - logsig2_w: Parameter for log variance of weights\n",
    "        # - random: tensor for storing random noise (not a parameter)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, inpt):\n",
    "        # TODO: Implement the forward pass\n",
    "        # 1. Generate new random numbers if self.resample is True\n",
    "        # 2. Compute the variance from log variance\n",
    "        # 3. Apply reparameterization trick to get weights\n",
    "        # 4. Apply linear transformation\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # TODO: Initialize parameters\n",
    "        # Initialize mu_w with normal distribution\n",
    "        # Initialize logsig2_w with small values around -9\n",
    "        # Initialize bias to zero\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "    def KL(self, loguniform=False):\n",
    "        # TODO: Compute KL divergence\n",
    "        # Formula: 0.5 * sum(μ² + σ² - log(σ²) - 1)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Test the VBLinear Layer\n",
    "\n",
    "Test your implementation by creating two VBLinear layers and passing data through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m tlr1(tlr0(x))\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test stochasticity - run multiple times\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting stochasticity (should be different each time):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Test your VBLinear implementation\n",
    "tlr0 = VBLinear(10, 5)\n",
    "tlr1 = VBLinear(5, 2)\n",
    "\n",
    "# Test data\n",
    "x = torch.rand(20, 10)\n",
    "\n",
    "# Forward pass\n",
    "output = tlr1(tlr0(x))\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Test stochasticity - run multiple times\n",
    "print(\"\\nTesting stochasticity (should be different each time):\")\n",
    "for i in range(5):\n",
    "    print(tlr0(x)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement the Bayesian Loss Function\n",
    "\n",
    "The Bayesian loss function combines two components:\n",
    "1. Negative log Gaussian likelihood\n",
    "2. KL divergence from the network weights\n",
    "\n",
    "Implement the negative log Gaussian function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_gauss(outputs, targets):\n",
    "    # TODO: Implement negative log Gaussian\n",
    "    # outputs[:, 0] contains the predicted means\n",
    "    # outputs[:, 1] contains the log variance predictions\n",
    "    # Formula: (μ - y)² / (2σ²) + 0.5 * log(σ²)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Build the Bayesian Neural Network\n",
    "\n",
    "Complete the implementation of the Bayesian neural network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50\n",
    "\n",
    "class bayes_amp_net(Module):\n",
    "    \n",
    "    def __init__(self, training_size, hdn_dim=50):\n",
    "        super(bayes_amp_net, self).__init__()\n",
    "        \n",
    "        self.training_size = training_size\n",
    "        self.vb_layers = []\n",
    "        self.all_layers = []\n",
    "\n",
    "        # TODO: Build the network architecture\n",
    "        # 1. Input layer: VBLinear(ipt_dim, hdn_dim) + ReLU\n",
    "        # 2. Two hidden layers: VBLinear(hdn_dim, hdn_dim) + ReLU each\n",
    "        # 3. Output layer: VBLinear(hdn_dim, 2) for mean and log variance\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "        self.model = nn.Sequential(*self.all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def KL(self):\n",
    "        # TODO: Compute total KL divergence from all VB layers\n",
    "        # Don't forget to normalize by training_size\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def neg_log_gauss(self, outputs, targets):\n",
    "        # TODO: Use the neg_log_gauss function you implemented\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Training Loop\n",
    "\n",
    "Implement the training function for the Bayesian neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer):\n",
    "    model.train()\n",
    "    loss_tot = 0.0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # TODO: Implement training step\n",
    "        # 1. Forward pass through model\n",
    "        # 2. Compute negative log Gaussian loss\n",
    "        # 3. Compute KL divergence\n",
    "        # 4. Total loss = neg_log_gauss + KL\n",
    "        # 5. Backpropagation and optimizer step\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "    return loss_tot / len(dataloader)\n",
    "\n",
    "def validate_epoch(dataloader, model):\n",
    "    model.eval()\n",
    "    loss_tot = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            #TODO: Implement validation step\n",
    "            pass\n",
    "\n",
    "    return loss_tot / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Training and Evaluation\n",
    "\n",
    "Set up the training loop and train your Bayesian neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Initialize model\n",
    "trn_len = trn_amplp.shape[0]\n",
    "model = bayes_amp_net(trn_len, hdn_dim=50).to(device)\n",
    "batch_size = 128\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp, trn_amplp.unsqueeze(-1)).to(device)\n",
    "val_dataset = amp_dataset(val_datfp, val_amplp.unsqueeze(-1)).to(device)\n",
    "tst_dataset = amp_dataset(tst_datfp, tst_amplp.unsqueeze(-1)).to(device)\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# TODO: Set up optimizer and training parameters\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Implement training loop\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Uncertainty Quantification\n",
    "\n",
    "Implement functions to extract predictions and uncertainties from your trained Bayesian model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, dataloader, n_monte=30):\n",
    "    # TODO: Implement Monte Carlo sampling\n",
    "    # 1. Run the model multiple times (n_monte) to get different samples\n",
    "    # 2. Collect predictions for both mean and variance\n",
    "    # 3. Return samples for further analysis\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Analysis and Visualization\n",
    "\n",
    "Create visualizations to analyze your results:\n",
    "1. Plot training/validation loss curves\n",
    "2. Compare predicted vs true amplitudes\n",
    "3. Analyze uncertainty estimates\n",
    "4. Compute and plot pull distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement analysis and visualization\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
