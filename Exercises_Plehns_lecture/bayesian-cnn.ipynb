{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Bayesian Convolutional Neural Networks for Jet Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, you will learn to implement and train Bayesian Convolutional Neural Networks (Bayesian CNNs) for jet classification in particle physics. Bayesian neural networks provide uncertainty quantification by treating network weights as probability distributions rather than fixed values.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the principles of Bayesian deep learning\n",
    "- Implement Bayesian convolutional layers using the reparameterization trick\n",
    "- Train a Bayesian CNN for binary classification of particle jets\n",
    "- Quantify predictive uncertainty using Monte Carlo sampling\n",
    "- Evaluate model performance with ROC curves and uncertainty analysis\n",
    "\n",
    "### Background\n",
    "Particle jets are collimated sprays of particles produced in high-energy collisions. Distinguishing between signal jets (e.g., from top quark decay) and background jets is crucial for many physics analyses. Bayesian neural networks not only provide classification predictions but also quantify uncertainty in those predictions, which is valuable for physics applications where understanding model confidence is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Outline\n",
    "\n",
    "1. **Data Loading and Preprocessing**\n",
    "   - Load jet constituent data\n",
    "   - Convert particle constituents to image representation\n",
    "   - Preprocess jet images (centering, rotation, normalization)\n",
    "\n",
    "2. **Bayesian Neural Network Components**\n",
    "   - Implement variational Bayesian linear layers\n",
    "   - Build Bayesian convolutional layers using reparameterization trick\n",
    "   - Understand KL divergence regularization\n",
    "\n",
    "3. **Model Architecture**\n",
    "   - Design Bayesian CNN for jet classification\n",
    "   - Combine Bayesian and standard layers\n",
    "\n",
    "4. **Training and Evaluation**\n",
    "   - Implement Bayesian loss function (negative log-likelihood + KL divergence)\n",
    "   - Train the model with proper regularization\n",
    "   - Evaluate classification performance\n",
    "\n",
    "5. **Uncertainty Quantification**\n",
    "   - Use Monte Carlo sampling for uncertainty estimation\n",
    "   - Analyze prediction uncertainty patterns\n",
    "   - Visualize uncertainty vs prediction confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Setup\n",
    "\n",
    "### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init, Module\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Load the data using the correct path you set previously for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!python3 data/get_data.py 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( \"data/toptagging-short/x_train_short.npy\")\n",
    "y_train = np.load( \"data/toptagging-short/y_train_short.npy\")\n",
    "X_test = np.load( \"data/toptagging-short/x_test_short.npy\")\n",
    "y_test = np.load( \"data/toptagging-short/y_test_short.npy\")\n",
    "X_val = np.load( \"data/toptagging-short/x_val_short.npy\")\n",
    "y_val = np.load( \"data/toptagging-short/y_val_short.npy\")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "The preprocessing functions are provided to convert particle constituents into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid settings for image creation\n",
    "xpixels = np.arange(-2.6, 2.6, 0.029)  # eta range\n",
    "ypixels = np.arange(-np.pi, np.pi, 0.035)  # phi range\n",
    "\n",
    "def eta(pT, pz):\n",
    "    \"\"\"Calculate pseudorapidity from transverse momentum and z-momentum\"\"\"\n",
    "    small = 1e-10\n",
    "    small_pT = (np.abs(pT) < small)\n",
    "    small_pz = (np.abs(pz) < small)\n",
    "    not_small = ~(small_pT | small_pz)\n",
    "    theta = np.arctan(pT[not_small]/pz[not_small])\n",
    "    theta[theta < 0] += np.pi\n",
    "    etas = np.zeros_like(pT)\n",
    "    etas[small_pz] = 0\n",
    "    etas[small_pT] = 1e-10\n",
    "    etas[not_small] = -np.log(np.tan(theta/2))\n",
    "    return etas\n",
    "\n",
    "def phi(px, py):\n",
    "    \"\"\"Calculate azimuthal angle from x and y momentum components\"\"\"\n",
    "    phis = np.arctan2(py, px)\n",
    "    phis[phis < 0] += 2*np.pi\n",
    "    phis[phis > 2*np.pi] -= 2*np.pi\n",
    "    phis = phis - np.pi \n",
    "    return phis\n",
    "\n",
    "def mass(E, px, py, pz):\n",
    "    \"\"\"Calculate invariant mass from 4-momentum\"\"\"\n",
    "    return np.sqrt(np.maximum(0., E**2 - px**2 - py**2 - pz**2))\n",
    "\n",
    "def img_mom(x, y, weights, x_power, y_power):\n",
    "    \"\"\"Calculate image moments for centering and rotation\"\"\"\n",
    "    return ((x**x_power) * (y**y_power) * weights).sum()\n",
    "\n",
    "def orig_image(etas, phis, es):\n",
    "    \"\"\"Create 2D histogram image from eta, phi coordinates and energies\"\"\"\n",
    "    z = np.zeros((etas.shape[0], len(xpixels), len(ypixels)))\n",
    "    in_grid = ~((etas < xpixels[0]) | (etas > xpixels[-1]) | \n",
    "                (phis < ypixels[0]) | (phis > ypixels[-1]))\n",
    "    \n",
    "    xcoords = np.argmin(np.abs(etas[:, None, :] - xpixels[None, :, None]), axis=1)\n",
    "    ycoords = np.argmin(np.abs(phis[:, None, :] - ypixels[None, :, None]), axis=1)\n",
    "    ncoords = np.repeat(np.arange(etas.shape[0])[:, None], etas.shape[1], axis=1)\n",
    "    \n",
    "    z[ncoords[in_grid], ycoords[in_grid], xcoords[in_grid]] = es[in_grid]\n",
    "    return z\n",
    "\n",
    "def preprocessing(x, y, weights, rotate=True, flip=True):\n",
    "    \"\"\"Preprocess jet image: center, rotate, and flip for standardization\"\"\"\n",
    "    # Center the image\n",
    "    x_centroid = img_mom(x, y, weights, 1, 0) / weights.sum()\n",
    "    y_centroid = img_mom(x, y, weights, 0, 1) / weights.sum()\n",
    "    x = x - x_centroid\n",
    "    y = y - y_centroid\n",
    "\n",
    "    if rotate:\n",
    "        # Calculate covariance matrix for rotation\n",
    "        u11 = img_mom(x, y, weights, 1, 1) / weights.sum()\n",
    "        u20 = img_mom(x, y, weights, 2, 0) / weights.sum()\n",
    "        u02 = img_mom(x, y, weights, 0, 2) / weights.sum()\n",
    "        cov = np.array([[u20, u11], [u11, u02]])\n",
    "\n",
    "        # Find principal axis\n",
    "        evals, evecs = np.linalg.eig(cov)\n",
    "        sort_indices = np.argsort(evals)[::-1]\n",
    "        e_1 = evecs[:, sort_indices[0]]\n",
    "\n",
    "        # Rotate so principal axis is vertical\n",
    "        theta = np.arctan2(e_1[0], e_1[1])\n",
    "        rotation = np.matrix([[np.cos(theta), -np.sin(theta)], \n",
    "                             [np.sin(theta), np.cos(theta)]])\n",
    "        transformed_mat = rotation * np.stack([x, y])\n",
    "        x_rot, y_rot = transformed_mat.A\n",
    "    else: \n",
    "        x_rot, y_rot = x, y\n",
    "  \n",
    "    # Flip for consistency\n",
    "    if flip:\n",
    "        if weights[x_rot < 0.].sum() < weights[x_rot > 0.].sum():\n",
    "            x_rot = -x_rot\n",
    "        if weights[y_rot < 0.].sum() > weights[y_rot > 0.].sum():\n",
    "            y_rot = -y_rot\n",
    "            \n",
    "    return x_rot, y_rot\n",
    "\n",
    "def constit_to_img(jets, n_constit, norm, rotate, flip):\n",
    "    \"\"\"Convert jet constituents to preprocessed images\"\"\"\n",
    "    print(\"Crop constituents\")\n",
    "    jets = jets[:, 0:n_constit, :]\n",
    "    \n",
    "    print(\"Calculating kinematic variables\")\n",
    "    E = jets[:, :, 0]\n",
    "    pxs = jets[:, :, 1]\n",
    "    pys = jets[:, :, 2]\n",
    "    pzs = jets[:, :, 3]\n",
    "    pT = np.sqrt(pxs**2 + pys**2)\n",
    "    \n",
    "    etas = eta(pT, pzs)\n",
    "    phis = phi(pxs, pys)\n",
    "    \n",
    "    # Pre-shift phis to handle periodicity\n",
    "    print(\"Pre-shifting phis\")\n",
    "    phis = (phis.T - phis[:, 0]).T\n",
    "    phis[phis < -np.pi] += 2*np.pi\n",
    "    phis[phis > np.pi] -= 2*np.pi\n",
    "    \n",
    "    weights = pT  # Use pT as weights\n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    for i in range(np.shape(etas)[0]):\n",
    "        etas[i, :], phis[i, :] = preprocessing(etas[i, :], phis[i, :], \n",
    "                                              weights[i, :], rotate, flip)\n",
    "    \n",
    "    print(\"Creating images\")\n",
    "    z_ori = orig_image(etas, phis, weights)\n",
    "        \n",
    "    print(\"Cropping and normalizing\")\n",
    "    n_crop = 40\n",
    "    z_new = np.zeros((z_ori.shape[0], n_crop, n_crop))\n",
    "    for i in range(z_ori.shape[0]):\n",
    "        Npix = z_ori[i, :, :].shape\n",
    "        z_new[i, :, :] = z_ori[i, \n",
    "                               int(Npix[0]/2 - n_crop/2):int(Npix[0]/2 + n_crop/2), \n",
    "                               int(Npix[1]/2 - n_crop/2):int(Npix[1]/2 + n_crop/2)]\n",
    "        if norm:\n",
    "            z_sum = z_new[i, :, :].sum()\n",
    "            if z_sum != 0.:\n",
    "                z_new[i, :, :] = z_new[i, :, :] / z_sum\n",
    "    \n",
    "    print(\"Reshaping\")\n",
    "    z_out = z_new.reshape((z_new.shape[0], -1))\n",
    "    \n",
    "    return z_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Constituents to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert constituent data to images\n",
    "z_train = constit_to_img(X_train, 50, True, True, True)\n",
    "z_test = constit_to_img(X_test, 50, True, True, True)\n",
    "z_val = constit_to_img(X_val, 50, True, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Average Jet Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate signal and background for visualization\n",
    "sig = z_train[np.where(y_train == 1)]\n",
    "bkg = z_train[np.where(y_train == 0)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot average signal jet\n",
    "axs[0].imshow(sig.mean(0).reshape((40, 40)), cmap=\"gist_heat_r\")\n",
    "axs[0].set_title(\"Average Signal Jet (Top)\")\n",
    "axs[0].set_xlabel(\"$\\\\eta$\")\n",
    "axs[0].set_ylabel(\"$\\\\phi$\")\n",
    "\n",
    "# Plot average background jet\n",
    "axs[1].imshow(bkg.mean(0).reshape((40, 40)), cmap=\"gist_heat_r\")\n",
    "axs[1].set_title(\"Average Background Jet (QCD)\")\n",
    "axs[1].set_xlabel(\"$\\\\eta$\")\n",
    "axs[1].set_ylabel(\"$\\\\phi$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNData(Dataset):\n",
    "    \"\"\"Custom dataset for CNN training\"\"\"\n",
    "    \n",
    "    def __init__(self, imgs, labels):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN input and convert to tensors\n",
    "z_train = torch.Tensor(z_train.reshape(-1, 1, 40, 40).astype('float32'))\n",
    "z_test = torch.Tensor(z_test.reshape(-1, 1, 40, 40).astype('float32'))\n",
    "z_val = torch.Tensor(z_val.reshape(-1, 1, 40, 40).astype('float32'))\n",
    "\n",
    "y_train = torch.Tensor(y_train).unsqueeze(-1)\n",
    "y_test = torch.Tensor(y_test).unsqueeze(-1)\n",
    "y_val = torch.Tensor(y_val).unsqueeze(-1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CNNData(z_train, y_train.float())\n",
    "test_dataset = CNNData(z_test, y_test.float())\n",
    "val_dataset = CNNData(z_val, y_val.float())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Data shapes - Train: {z_train.shape}, Val: {z_val.shape}, Test: {z_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian Neural Network Components\n",
    "\n",
    "### Variational Bayesian Linear Layer\n",
    "\n",
    "We give the code for a variational Bayesian linear layer, as seen in the tutorial for bayesian regression.\n",
    "\n",
    "**Key concepts:**\n",
    "- Weights are distributions: w ~ N(μ, σ²)\n",
    "- Reparameterization trick: w = μ + σ * ε, where ε ~ N(0,1)\n",
    "- KL divergence measures distance from prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLinear(Module):\n",
    "    \"\"\"Variational Bayesian Linear Layer\n",
    "    \n",
    "    Implements a linear layer where weights are treated as distributions.\n",
    "    Uses reparameterization trick: w = μ + σ * ε, where ε ~ N(0,1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(VBLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.resample = True\n",
    "        \n",
    "        # Parameters for weight distribution: w ~ N(μ, σ²)\n",
    "        self.bias = Parameter(Tensor(out_features))\n",
    "        self.mu_w = Parameter(Tensor(out_features, in_features))  # Weight means\n",
    "        self.logsig2_w = Parameter(Tensor(out_features, in_features))  # Log weight variances\n",
    "        \n",
    "        # Random noise for reparameterization\n",
    "        self.random = torch.randn_like(self.logsig2_w)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.resample:\n",
    "            self.random = torch.randn_like(self.logsig2_w)\n",
    "        \n",
    "        # Reparameterization trick: w = μ + σ * ε\n",
    "        s2_w = self.logsig2_w.exp()  # Convert log variance to variance\n",
    "        weight = self.mu_w + s2_w.sqrt() * self.random\n",
    "        \n",
    "        return F.linear(input, weight, self.bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters\"\"\"\n",
    "        stdv = 1. / np.sqrt(self.mu_w.size(1))\n",
    "        self.mu_w.data.normal_(0, stdv)\n",
    "        self.logsig2_w.data.zero_().normal_(-9, 0.001)  # Start with small variance\n",
    "        self.bias.data.zero_()\n",
    "        \n",
    "    def KL(self, loguniform=False):\n",
    "        \"\"\"Calculate KL divergence from prior N(0,1)\"\"\"\n",
    "        return 0.5 * (self.mu_w.pow(2) + self.logsig2_w.exp() - self.logsig2_w - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Convolutional Layer\n",
    "\n",
    "Implement the BBBConv2d class, which is a Bayesian version of the Conv2d layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.common_types import _size_2_t\n",
    "from typing import Optional, Union\n",
    "\n",
    "def KL_DIV(mu_q, sig_q, mu_p, sig_p):\n",
    "    \"\"\"Calculate KL divergence between two Gaussians\"\"\"\n",
    "    kl = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 + \n",
    "                (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n",
    "    return kl\n",
    "\n",
    "class BBBConv2d(Module):\n",
    "    \"\"\"Bayes by Backprop Convolutional Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, bias=True, priors=None):\n",
    "        super(BBBConv2d, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = 1\n",
    "        self.use_bias = bias\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Prior and posterior parameters\n",
    "        if priors is None:\n",
    "            priors = {\n",
    "                'prior_mu': 0,\n",
    "                'prior_sigma': 0.1,\n",
    "                'posterior_mu_initial': (0, 0.1),\n",
    "                'posterior_rho_initial': (-3, 0.1),\n",
    "            }\n",
    "        self.prior_mu = priors['prior_mu']\n",
    "        self.prior_sigma = priors['prior_sigma']\n",
    "        self.posterior_mu_initial = priors['posterior_mu_initial']\n",
    "        self.posterior_rho_initial = priors['posterior_rho_initial']\n",
    "\n",
    "        # Weight parameters: μ and ρ (where σ = log(1 + exp(ρ)))\n",
    "        #TODO: Implement the mu and rho parameters for weights\n",
    "        self.W_mu = # YOUR CODE HERE\n",
    "        self.W_rho = # YOUR CODE HERE\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = Parameter(torch.empty((out_channels), device=self.device))\n",
    "            self.bias_rho = Parameter(torch.empty((out_channels), device=self.device))\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_rho', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters\"\"\"\n",
    "        self.W_mu.data.normal_(*self.posterior_mu_initial)\n",
    "        self.W_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n",
    "            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "    def forward(self, input, sample=True):\n",
    "        \"\"\"Forward pass with weight sampling\"\"\"\n",
    "        if self.training or sample:\n",
    "            # Sample weights: w = μ + σ * ε\n",
    "            #TODO: Implement the sampling of weights\n",
    "            W_eps = # YOUR CODE HERE\n",
    "            self.W_sigma = # YOUR CODE HERE: Compute σ = log(1 + exp(ρ))\n",
    "            weight = None # YOUR CODE HERE: Compute w = μ + σ * ε\n",
    "\n",
    "            if self.use_bias:\n",
    "                bias_eps = torch.empty(self.bias_mu.size()).normal_(0, 1).to(self.device)\n",
    "                self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "                bias = self.bias_mu + bias_eps * self.bias_sigma\n",
    "            else:\n",
    "                bias = None\n",
    "        else:\n",
    "            # Use mean weights for deterministic inference\n",
    "            weight = # YOUR CODE HERE\n",
    "            bias = # YOUR CODE HERE if self.use_bias else None\n",
    "\n",
    "        return F.conv2d(input, weight, bias, self.stride, \n",
    "                       self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def KL(self):\n",
    "        \"\"\"Calculate KL divergence from prior\"\"\"\n",
    "        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n",
    "        if self.use_bias:\n",
    "            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bayesian CNN Architecture\n",
    "\n",
    "**Exercise**: Complete the Bayesian CNN architecture. Pay attention to which layers are Bayesian and which are standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesConvNet2D(nn.Module):\n",
    "    \"\"\"Bayesian Convolutional Neural Network for Jet Classification\"\"\"\n",
    "    \n",
    "    def __init__(self, training_size=None, in_ch=1, ch=4, out_dim=1, img_sz=40):\n",
    "        super().__init__()\n",
    "        self.training_size = training_size\n",
    "        \n",
    "        # Track Bayesian layers for KL calculation\n",
    "        self.vb_layers = []\n",
    "        self.all_layers = []\n",
    "        \n",
    "        # TODO: First Bayesian convolutional block\n",
    "        self.conv1 = # YOUR CODE HERE - BBBConv2d layer\n",
    "        self.vb_layers.append(self.conv1)\n",
    "        self.all_layers.append(self.conv1)\n",
    "        self.all_layers.append(nn.ReLU())\n",
    "        \n",
    "        # First pooling layer\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.all_layers.append(self.max1)\n",
    "        self.all_layers.append(nn.ReLU())\n",
    "        \n",
    "        # TODO: Second Bayesian convolutional block\n",
    "        self.conv2 = # YOUR CODE HERE - BBBConv2d layer\n",
    "        self.vb_layers.append(self.conv2)\n",
    "        self.all_layers.append(self.conv2)\n",
    "        \n",
    "        # Second pooling layer\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.all_layers.append(self.max2)\n",
    "        \n",
    "        # Flatten for fully connected layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.all_layers.append(self.flatten)\n",
    "        \n",
    "        # TODO: Bayesian fully connected output layer\n",
    "        self.out = # YOUR CODE HERE - VBLinear layer\n",
    "        self.vb_layers.append(self.out)\n",
    "        self.all_layers.append(self.out)\n",
    "        \n",
    "        # Sigmoid activation for binary classification\n",
    "        self.all_layers.append(nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        self.net = nn.Sequential(*self.all_layers)\n",
    "        return self.net(x)\n",
    "    \n",
    "    def KL(self):\n",
    "        \"\"\"Calculate total KL divergence from all Bayesian layers\"\"\"\n",
    "        # TODO: Sum KL divergence from all Bayesian layers\n",
    "        kl = 0\n",
    "        for vb_layer in self.vb_layers:\n",
    "            kl += # YOUR CODE HERE\n",
    "        return kl / self.training_size  # Normalize by training set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Test the network with a single sample\n",
    "test_net = BayesConvNet2D()\n",
    "sample_input = train_dataset[0][0].unsqueeze(0)\n",
    "sample_output = test_net(sample_input)\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "print(f\"Sample output: {sample_output.item():.4f}\")\n",
    "print(f\"Sample KL divergence: {test_net.KL().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions\n",
    "\n",
    "**Exercise**: Complete the training function. The key is implementing the Bayesian loss = NLL + KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        \n",
    "        # TODO: Calculate Bayesian loss\n",
    "        loss_fn = nn.BCELoss()\n",
    "        nll = # YOUR CODE HERE - Negative log-likelihood\n",
    "        kl = # YOUR CODE HERE - KL divergence regularization\n",
    "        loss = # YOUR CODE HERE - Total Bayesian loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"Loss: {loss:>8f} KL: {kl:>8f} NLL: {nll:>8f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, model, dataset_name=\"\"):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    total_nll, total_kl, total_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            \n",
    "            loss_fn = nn.BCELoss()\n",
    "            nll = loss_fn(pred, y)\n",
    "            kl = model.KL()\n",
    "            loss = nll + kl\n",
    "            \n",
    "            total_nll += nll.item()\n",
    "            total_kl += kl.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_nll = total_nll / num_batches\n",
    "    avg_kl = total_kl / num_batches\n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    print(f\"{dataset_name} Loss: {avg_loss:>8f} KL: {avg_kl:>8f} NLL: {avg_nll:>8f}\")\n",
    "    return avg_nll, avg_kl, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "**Exercise**: Set up and run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separator():\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 50\n",
    "learning_rate = 5e-4\n",
    "trn_len = len(y_train)\n",
    "\n",
    "print(f\"Training dataset size: {trn_len}\")\n",
    "\n",
    "# TODO: Initialize model and optimizer\n",
    "model = # YOUR CODE HERE\n",
    "optimizer = # YOUR CODE HERE\n",
    "\n",
    "separator()\n",
    "print(\"Model Architecture:\")\n",
    "separator()\n",
    "print(model)\n",
    "separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses, val_losses = [], []\n",
    "train_nlls, val_nlls = [], []\n",
    "train_kls, val_kls = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    separator()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    separator()\n",
    "    \n",
    "    # Training\n",
    "    train_epoch(train_dataloader, model, optimizer)\n",
    "    \n",
    "    # Evaluation\n",
    "    train_nll, train_kl, train_loss = evaluate_model(train_dataloader, model, \"Train\")\n",
    "    val_nll, val_kl, val_loss = evaluate_model(val_dataloader, model, \"Val\")\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_nlls.append(train_nll)\n",
    "    val_nlls.append(val_nll)\n",
    "    train_kls.append(train_kl)\n",
    "    val_kls.append(val_kl)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Analysis\n",
    "\n",
    "**Exercise**: Implement Monte Carlo sampling for uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x, model, n_monte=20):\n",
    "    \"\"\"Get predictions with uncertainty using Monte Carlo sampling\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_monte):\n",
    "        print(f\"Monte Carlo sample {i+1}/{n_monte}\")\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get prediction and extract first column\n",
    "            pred = # YOUR CODE HERE\n",
    "            predictions.append(pred)\n",
    "        \n",
    "    predictions = torch.stack(predictions)\n",
    "    # TODO: Calculate mean and standard deviation\n",
    "    mean = # YOUR CODE HERE\n",
    "    std = # YOUR CODE HERE\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set with uncertainty quantification\n",
    "samp_size = 2000  # Use subset for faster evaluation\n",
    "\n",
    "# Get predictions with uncertainty\n",
    "test_pred, test_std = get_prediction(z_test[:samp_size], model, n_monte=10)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_pred_labels = torch.round(test_pred[:samp_size])\n",
    "test_correct = (test_pred_labels == y_test[:samp_size]).sum().item()\n",
    "test_accuracy = test_correct / len(test_pred_labels) * 100\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Total loss\n",
    "axs[0].plot(train_losses, label=\"Train Loss\", color='tab:red')\n",
    "axs[0].plot(val_losses, label=\"Val Loss\", color='tab:green')\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Total Loss\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Negative log-likelihood\n",
    "axs[1].plot(train_nlls, label=\"Train NLL\", color='tab:red')\n",
    "axs[1].plot(val_nlls, label=\"Val NLL\", color='tab:green')\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Negative Log-Likelihood\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# KL divergence\n",
    "axs[2].plot(train_kls, label=\"Train KL\", color='tab:red')\n",
    "axs[2].plot(val_kls, label=\"Val KL\", color='tab:green')\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].set_ylabel(\"KL Divergence\")\n",
    "axs[2].legend()\n",
    "axs[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Analysis and Uncertainty Visualization\n",
    "\n",
    "**Exercise**: Complete the uncertainty analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC metrics\n",
    "fpr, tpr, thresholds = roc_curve(y_test[:samp_size].numpy(), test_pred.numpy())\n",
    "auc_score = roc_auc_score(y_test[:samp_size].numpy(), test_pred.numpy())\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.3f}\")\n",
    "\n",
    "# TODO: Analyze uncertainty patterns\n",
    "test_std_np = # YOUR CODE HERE - Convert uncertainty to numpy\n",
    "test_pred_np = # YOUR CODE HERE - Convert predictions to numpy\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# TODO: Plot prediction distribution\n",
    "# TODO: Plot uncertainty distribution  \n",
    "# TODO: Plot uncertainty vs prediction scatter plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUncertainty Statistics:\")\n",
    "print(f\"Mean uncertainty: {test_std_np.mean():.4f}\")\n",
    "print(f\"Max uncertainty: {test_std_np.max():.4f}\")\n",
    "print(f\"Min uncertainty: {test_std_np.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "In this tutorial, you have learned:\n",
    "\n",
    "1. **Bayesian Deep Learning Fundamentals**: How to treat neural network weights as probability distributions rather than fixed values.\n",
    "\n",
    "2. **Implementation Details**: \n",
    "   - Variational Bayesian layers using the reparameterization trick\n",
    "   - Bayesian convolutional layers for image processing\n",
    "   - KL divergence regularization for proper weight distribution learning\n",
    "\n",
    "3. **Training Bayesian Networks**: How to combine negative log-likelihood with KL divergence for the total loss function.\n",
    "\n",
    "4. **Uncertainty Quantification**: Using Monte Carlo sampling to estimate predictive uncertainty, which is crucial for physics applications.\n",
    "\n",
    "5. **Performance Evaluation**: Standard classification metrics (ROC curves, AUC) combined with uncertainty analysis.\n",
    "\n",
    "### Key Advantages of Bayesian CNNs:\n",
    "- **Uncertainty Quantification**: Know when the model is uncertain\n",
    "- **Regularization**: KL divergence acts as automatic regularization\n",
    "- **Robustness**: Better handling of out-of-distribution samples\n",
    "\n",
    "### Applications in Particle Physics:\n",
    "- **Systematic Uncertainty Estimation**: Model uncertainty contributes to systematic uncertainties\n",
    "- **Data Quality Assessment**: High uncertainty regions may indicate problematic data\n",
    "- **Decision Making**: Uncertainty-aware decisions in analysis pipelines\n",
    "\n",
    "This framework can be extended to other particle physics applications such as particle identification, trigger systems, and analysis-specific classifiers where uncertainty quantification is valuable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
